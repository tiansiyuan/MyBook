{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1050,
     "status": "ok",
     "timestamp": 1545619520957,
     "user": {
      "displayName": "Victor Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/-nVI9QJN9Uk4/AAAAAAAAAAI/AAAAAAAAACk/6lxg_oQk5wU/s64/photo.jpg",
      "userId": "17175242154365071029"
     },
     "user_tz": -480
    },
    "id": "08X8cY6f3N1g",
    "outputId": "b5ef9362-05c5-4b9f-e47a-61bbcb25e05e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.6\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 807,
     "status": "ok",
     "timestamp": 1545619538250,
     "user": {
      "displayName": "Victor Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/-nVI9QJN9Uk4/AAAAAAAAAAI/AAAAAAAAACk/6lxg_oQk5wU/s64/photo.jpg",
      "userId": "17175242154365071029"
     },
     "user_tz": -480
    },
    "id": "2zcE7g3HGH9z",
    "outputId": "79b42f44-9cfa-4142-8f28-9bd2fd104576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7apeEevGIEK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 867,
     "status": "ok",
     "timestamp": 1545630718236,
     "user": {
      "displayName": "Victor Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/-nVI9QJN9Uk4/AAAAAAAAAAI/AAAAAAAAACk/6lxg_oQk5wU/s64/photo.jpg",
      "userId": "17175242154365071029"
     },
     "user_tz": -480
    },
    "id": "y899qUaV3N4X",
    "outputId": "513c7974-32ed-47fb-89b0-a09971fd3f86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go.\\tVa !\\nHi.\\tSalut !\\nRun!\\tCours\\u202f!\\nRun!\\tCourez\\u202f!\\nWow!\\tÇa alors\\u202f!\\nFire!\\tAu feu !\\nH'"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"fra.txt\", 'rt', encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "text[:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5529,
     "status": "ok",
     "timestamp": 1545631274659,
     "user": {
      "displayName": "Victor Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/-nVI9QJN9Uk4/AAAAAAAAAAI/AAAAAAAAACk/6lxg_oQk5wU/s64/photo.jpg",
      "userId": "17175242154365071029"
     },
     "user_tz": -480
    },
    "id": "F1sl6tid3N90",
    "outputId": "f5c8d3e6-d5fe-40bf-c8d7-a43c5056a9a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Go.', 'Va !'],\n",
       " ['Hi.', 'Salut !'],\n",
       " ['Run!', 'Cours\\u202f!'],\n",
       " ['Run!', 'Courez\\u202f!'],\n",
       " ['Wow!', 'Ça alors\\u202f!'],\n",
       " ['Fire!', 'Au feu !'],\n",
       " ['Help!', \"À l'aide\\u202f!\"],\n",
       " ['Jump.', 'Saute.'],\n",
       " ['Stop!', 'Ça suffit\\u202f!'],\n",
       " ['Stop!', 'Stop\\u202f!'],\n",
       " ['Stop!', 'Arrête-toi !'],\n",
       " ['Wait!', 'Attends !'],\n",
       " ['Wait!', 'Attendez !'],\n",
       " ['Go on.', 'Poursuis.'],\n",
       " ['Go on.', 'Continuez.']]"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = text.strip().split('\\n')\n",
    "lines_pairs = [line.split('\\t') for line in  lines]\n",
    "lines_pairs[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5087,
     "status": "ok",
     "timestamp": 1545631302234,
     "user": {
      "displayName": "Victor Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/-nVI9QJN9Uk4/AAAAAAAAAAI/AAAAAAAAACk/6lxg_oQk5wU/s64/photo.jpg",
      "userId": "17175242154365071029"
     },
     "user_tz": -480
    },
    "id": "uq3uYxv2CjOw",
    "outputId": "967075fa-c610-4c42-db3e-cdd0635f6fa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共有160872个法英文语料库对；\n",
      "其中法文语句最短的长度有4，最长的长度有349；\n",
      "其中英文语句最短的长度有3，最长的长度有286。\n"
     ]
    }
   ],
   "source": [
    "pairs_len = len(lines_pairs)\n",
    "eng_pair_lens = [len(line_pair[0]) for line_pair in lines_pairs]\n",
    "fra_pair_lens = [len(line_pair[1]) for line_pair in lines_pairs]\n",
    "print(\"一共有{}个法英文语料库对；\".format(pairs_len))\n",
    "print(\"其中法文语句最短的长度有{}，最长的长度有{}；\".format(min(fra_pair_lens), max(fra_pair_lens)))\n",
    "print(\"其中英文语句最短的长度有{}，最长的长度有{}。\".format(min(eng_pair_lens), max(eng_pair_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5901,
     "status": "ok",
     "timestamp": 1545633623422,
     "user": {
      "displayName": "Victor Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/-nVI9QJN9Uk4/AAAAAAAAAAI/AAAAAAAAACk/6lxg_oQk5wU/s64/photo.jpg",
      "userId": "17175242154365071029"
     },
     "user_tz": -480
    },
    "id": "bUVJx_5x3OAf",
    "outputId": "3e00692f-d660-460f-cc26-a533245b8e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['go', 'va'],\n",
       "       ['hi', 'salut'],\n",
       "       ['run', 'cours'],\n",
       "       ['run', 'courez'],\n",
       "       ['wow', 'ca alors'],\n",
       "       ['fire', 'au feu'],\n",
       "       ['help', 'a laide'],\n",
       "       ['jump', 'saute'],\n",
       "       ['stop', 'ca suffit'],\n",
       "       ['stop', 'stop'],\n",
       "       ['stop', 'arretetoi'],\n",
       "       ['wait', 'attends'],\n",
       "       ['wait', 'attendez'],\n",
       "       ['go on', 'poursuis'],\n",
       "       ['go on', 'continuez']], dtype='<U339')"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from unicodedata import normalize\n",
    "import string\n",
    "\n",
    "re_print = re.compile('[^{}]'.format(re.escape(string.printable)))\n",
    "english_table = str.maketrans('', '', string.punctuation)\n",
    "cleaned_pairs = list()\n",
    "for pair in lines_pairs:\n",
    "    clean_pair = list()\n",
    "    for i, line in enumerate(pair):\n",
    "          line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "          line = line.decode('UTF-8')\n",
    "          line = line.split()  \n",
    "          line = [word.lower() for word in line] \n",
    "          line = [word.translate(english_table) for word in line] \n",
    "          line = [re_print.sub('', w) for w in line] \n",
    "          line = [word for word in line if word.isalpha()] \n",
    "          clean_pair.append(' '.join(line))\n",
    "    cleaned_pairs.append(clean_pair) \n",
    "cleaned_pairs = np.array(cleaned_pairs) \n",
    "cleaned_pairs[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 945,
     "status": "ok",
     "timestamp": 1545634783740,
     "user": {
      "displayName": "Victor Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/-nVI9QJN9Uk4/AAAAAAAAAAI/AAAAAAAAACk/6lxg_oQk5wU/s64/photo.jpg",
      "userId": "17175242154365071029"
     },
     "user_tz": -480
    },
    "id": "RBWfkIiJ7JW0",
    "outputId": "16d54c2f-0646-4bdb-b1f5-f411cd20de19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\r",
      "\u000b",
      "\f",
      "\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.printable)\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ixpm3W_3OC9"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"french_to_english.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cleaned_pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4286,
     "status": "ok",
     "timestamp": 1545635491715,
     "user": {
      "displayName": "Victor Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/-nVI9QJN9Uk4/AAAAAAAAAAI/AAAAAAAAACk/6lxg_oQk5wU/s64/photo.jpg",
      "userId": "17175242154365071029"
     },
     "user_tz": -480
    },
    "id": "ZkZ7iRlm3OFm",
    "outputId": "6852a7cd-c74d-48f7-c53f-582c1644bdef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['he has a video', 'il detient une video'],\n",
       "       ['can you help me', 'pourraistu maider'],\n",
       "       ['be respectful', 'soyez respectueuses'],\n",
       "       ['youre naive', 'vous etes naif'],\n",
       "       ['he hardly works', 'il travaille a peine'],\n",
       "       ['show me', 'montrezmoi'],\n",
       "       ['i must go', 'je dois y aller'],\n",
       "       ['what a team', 'quelle equipe'],\n",
       "       ['we are late', 'nous sommes en retard'],\n",
       "       ['call security', 'appelle la securite'],\n",
       "       ['is that love', 'estce de lamour'],\n",
       "       ['he lay face up', 'il etait etendu le visage visible'],\n",
       "       ['she helps us', 'elle nous aide'],\n",
       "       ['she had twins', 'elle a eu des jumeaux'],\n",
       "       ['you needed me', 'vous aviez besoin de moi']], dtype='<U339')"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"french_to_english.pkl\", \"rb\") as f:\n",
    "    raw_dataset = pickle.load(f)\n",
    "\n",
    "sequence_length = 10000\n",
    "dataset = raw_dataset[:sequence_length]\n",
    "np.random.shuffle(dataset)\n",
    "dataset[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4723,
     "status": "ok",
     "timestamp": 1545635735016,
     "user": {
      "displayName": "Victor Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/-nVI9QJN9Uk4/AAAAAAAAAAI/AAAAAAAAACk/6lxg_oQk5wU/s64/photo.jpg",
      "userId": "17175242154365071029"
     },
     "user_tz": -480
    },
    "id": "aiqIH60a3OIB",
    "outputId": "9c14ce73-3487-4a4c-b78d-a73dd1afbf05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape=(8500, 2), test.shape=(1500, 2)\n"
     ]
    }
   ],
   "source": [
    "train_len = sequence_length - 1500\n",
    "train, test = dataset[:train_len], dataset[train_len:]\n",
    "\n",
    "def save_dataset(sentences, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(sentences, f)\n",
    "\n",
    "save_dataset(dataset, \"french_to_english_dataset_top10000.pkl\")\n",
    "save_dataset(train, \"french_to_english_train.pkl\")\n",
    "save_dataset(test, \"french_to_english_test.pkl\")\n",
    "\n",
    "print(\"train.shape={}, test.shape={}\".format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1014,
     "status": "ok",
     "timestamp": 1545621096645,
     "user": {
      "displayName": "Victor Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/-nVI9QJN9Uk4/AAAAAAAAAAI/AAAAAAAAACk/6lxg_oQk5wU/s64/photo.jpg",
      "userId": "17175242154365071029"
     },
     "user_tz": -480
    },
    "id": "Pu8mu2wxmH19",
    "outputId": "cc8b6830-80d1-4fd3-a468-00cc133485fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['we all laughed' 'nous rimes tous']\n",
      " ['is that you' 'estce toi']\n",
      " ['i like this' 'je lapprecie']\n",
      " ...\n",
      " ['i am not happy' 'je ne suis pas content']\n",
      " ['break it up' 'arretez']\n",
      " ['do it again' 'refaisle']]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 772,
     "status": "ok",
     "timestamp": 1545636788614,
     "user": {
      "displayName": "Victor Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/-nVI9QJN9Uk4/AAAAAAAAAAI/AAAAAAAAACk/6lxg_oQk5wU/s64/photo.jpg",
      "userId": "17175242154365071029"
     },
     "user_tz": -480
    },
    "id": "myO0ouzrJW2Z",
    "outputId": "2c8326d5-272a-4cae-ea12-8dd5251ed8ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.word_index={'ai': 1, 'i': 2, 'love': 3, 'in': 4, 'china': 5, '特拉字节': 6, '人工智能': 7}.\n",
      "tokenizer.texts_to_sequences=[[2, 3, 1, 4, 5], [6], [1, 7]].\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "texts = ['I love AI in China', '特拉字节', 'AI 人工智能']\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "print(\"tokenizer.word_index={}.\".format(tokenizer.word_index))\n",
    "print(\"tokenizer.texts_to_sequences={}.\".format(tokenizer.texts_to_sequences(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 863,
     "status": "ok",
     "timestamp": 1545638651411,
     "user": {
      "displayName": "Victor Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/-nVI9QJN9Uk4/AAAAAAAAAAI/AAAAAAAAACk/6lxg_oQk5wU/s64/photo.jpg",
      "userId": "17175242154365071029"
     },
     "user_tz": -480
    },
    "id": "xXb1i5o8ODAU",
    "outputId": "c009dc16-b4d2-4906-bd65-3c9ae8dfaa52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文序列单词最大个数5，单词有2125个。\n",
      "法文序列单词最大个数10，单词有4397个。\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    "\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('英文序列单词最大个数{}，单词有{}个。'.format(eng_length, eng_vocab_size))\n",
    "\n",
    "fra_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "fra_length = max_length(dataset[:, 1])\n",
    "print('法文序列单词最大个数{}，单词有{}个。'.format(fra_length, fra_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QIhAk7HKOt_z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "faz9K3uaHHHJ"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras import utils\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    X = sequence.pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = utils.to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = np.array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    "\n",
    "X_train = encode_sequences(fra_tokenizer, fra_length, train[:, 1])\n",
    "y_train = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "y_train = encode_output(y_train, eng_vocab_size)\n",
    " \n",
    "X_test = encode_sequences(fra_tokenizer, fra_length, test[:, 1])\n",
    "y_test = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "y_test = encode_output(y_test, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2517,
     "status": "ok",
     "timestamp": 1545640267718,
     "user": {
      "displayName": "Victor Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/-nVI9QJN9Uk4/AAAAAAAAAAI/AAAAAAAAACk/6lxg_oQk5wU/s64/photo.jpg",
      "userId": "17175242154365071029"
     },
     "user_tz": -480
    },
    "id": "4Xwut7hvHHJ7",
    "outputId": "4b7afb6a-41a1-430a-87f3-9e7e4663d63f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 256)           1125632   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 5, 2125)           546125    \n",
      "=================================================================\n",
      "Total params: 2,722,381\n",
      "Trainable params: 2,722,381\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, RepeatVector, TimeDistributed\n",
    "\n",
    "def create_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model\n",
    " \n",
    "model = create_model(fra_vocab_size, eng_vocab_size, fra_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HddmC6itAnqg"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K \n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2278
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1068280,
     "status": "ok",
     "timestamp": 1545641521624,
     "user": {
      "displayName": "Victor Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/-nVI9QJN9Uk4/AAAAAAAAAAI/AAAAAAAAACk/6lxg_oQk5wU/s64/photo.jpg",
      "userId": "17175242154365071029"
     },
     "user_tz": -480
    },
    "id": "9AjNZ8WMHHMm",
    "outputId": "fce25a33-6b6b-411a-8e9d-952f1881c2d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8500 samples, validate on 1500 samples\n",
      "Epoch 1/50\n",
      "8500/8500 [==============================] - 34s 4ms/step - loss: 4.3055 - val_loss: 3.3909\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.39091, saving model to translator_weights_model.h5\n",
      "Epoch 2/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 3.3292 - val_loss: 3.2612\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.39091 to 3.26117, saving model to translator_weights_model.h5\n",
      "Epoch 3/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 3.1713 - val_loss: 3.1294\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.26117 to 3.12938, saving model to translator_weights_model.h5\n",
      "Epoch 4/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 3.0185 - val_loss: 2.9979\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.12938 to 2.99794, saving model to translator_weights_model.h5\n",
      "Epoch 5/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 2.8298 - val_loss: 2.8720\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.99794 to 2.87198, saving model to translator_weights_model.h5\n",
      "Epoch 6/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 2.6585 - val_loss: 2.7729\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.87198 to 2.77288, saving model to translator_weights_model.h5\n",
      "Epoch 7/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 2.5037 - val_loss: 2.6703\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.77288 to 2.67034, saving model to translator_weights_model.h5\n",
      "Epoch 8/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 2.3579 - val_loss: 2.5719\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.67034 to 2.57190, saving model to translator_weights_model.h5\n",
      "Epoch 9/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 2.2078 - val_loss: 2.4696\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.57190 to 2.46964, saving model to translator_weights_model.h5\n",
      "Epoch 10/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 2.0630 - val_loss: 2.3806\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.46964 to 2.38057, saving model to translator_weights_model.h5\n",
      "Epoch 11/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 1.9193 - val_loss: 2.3084\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.38057 to 2.30842, saving model to translator_weights_model.h5\n",
      "Epoch 12/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 1.7873 - val_loss: 2.2346\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.30842 to 2.23463, saving model to translator_weights_model.h5\n",
      "Epoch 13/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 1.6575 - val_loss: 2.1718\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.23463 to 2.17175, saving model to translator_weights_model.h5\n",
      "Epoch 14/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 1.5366 - val_loss: 2.1196\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.17175 to 2.11962, saving model to translator_weights_model.h5\n",
      "Epoch 15/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 1.4204 - val_loss: 2.0686\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.11962 to 2.06864, saving model to translator_weights_model.h5\n",
      "Epoch 16/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 1.3129 - val_loss: 2.0330\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.06864 to 2.03299, saving model to translator_weights_model.h5\n",
      "Epoch 17/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 1.2091 - val_loss: 2.0062\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.03299 to 2.00623, saving model to translator_weights_model.h5\n",
      "Epoch 18/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 1.1126 - val_loss: 1.9709\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.00623 to 1.97086, saving model to translator_weights_model.h5\n",
      "Epoch 19/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 1.0206 - val_loss: 1.9361\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.97086 to 1.93607, saving model to translator_weights_model.h5\n",
      "Epoch 20/50\n",
      "8500/8500 [==============================] - 33s 4ms/step - loss: 0.9332 - val_loss: 1.9221\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.93607 to 1.92213, saving model to translator_weights_model.h5\n",
      "Epoch 21/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 0.8545 - val_loss: 1.8943\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.92213 to 1.89428, saving model to translator_weights_model.h5\n",
      "Epoch 22/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 0.7740 - val_loss: 1.8715\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.89428 to 1.87152, saving model to translator_weights_model.h5\n",
      "Epoch 23/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 0.7028 - val_loss: 1.8519\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.87152 to 1.85186, saving model to translator_weights_model.h5\n",
      "Epoch 24/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 0.6383 - val_loss: 1.8384\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.85186 to 1.83838, saving model to translator_weights_model.h5\n",
      "Epoch 25/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 0.5775 - val_loss: 1.8352\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.83838 to 1.83524, saving model to translator_weights_model.h5\n",
      "Epoch 26/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 0.5243 - val_loss: 1.8190\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.83524 to 1.81903, saving model to translator_weights_model.h5\n",
      "Epoch 27/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 0.4740 - val_loss: 1.8188\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.81903 to 1.81880, saving model to translator_weights_model.h5\n",
      "Epoch 28/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 0.4319 - val_loss: 1.8241\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.81880\n",
      "Epoch 29/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 0.3909 - val_loss: 1.8184\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.81880 to 1.81843, saving model to translator_weights_model.h5\n",
      "Epoch 30/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 0.3548 - val_loss: 1.8046\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.81843 to 1.80462, saving model to translator_weights_model.h5\n",
      "Epoch 31/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 0.3240 - val_loss: 1.8139\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.80462\n",
      "Epoch 32/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 0.2965 - val_loss: 1.8179\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.80462\n",
      "Epoch 33/50\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 0.2715 - val_loss: 1.8113\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.80462\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "callbacks_EarlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                                        patience=3)\n",
    "\n",
    "model_filename = 'translator_weights_model.h5'\n",
    "checkpoint_ModelCheckpoint = ModelCheckpoint(model_filename, \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    epochs=50, \n",
    "                    batch_size=64, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    callbacks=[checkpoint_ModelCheckpoint, callbacks_EarlyStopping], \n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1199,
     "status": "ok",
     "timestamp": 1545622237219,
     "user": {
      "displayName": "Victor Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/-nVI9QJN9Uk4/AAAAAAAAAAI/AAAAAAAAACk/6lxg_oQk5wU/s64/photo.jpg",
      "userId": "17175242154365071029"
     },
     "user_tz": -480
    },
    "id": "0FYj8FHgQzUa",
    "outputId": "65fcd65d-9aa6-4a8f-fb7c-2f6fb1987e97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'loss'])"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8k1Oc795HHPN"
   },
   "outputs": [],
   "source": [
    "def load_clean_sentences(filename):\n",
    "    return pickle.load(open(filename, 'rb'))\n",
    "\n",
    "dataset = load_clean_sentences('french_to_english_dataset_top10000.pkl')\n",
    "train_ds = load_clean_sentences('french_to_english_train.pkl')\n",
    "test_ds = load_clean_sentences('french_to_english_test.pkl')\n",
    "\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "fra_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "fra_length = max_length(dataset[:, 1])\n",
    "\n",
    "X_train = encode_sequences(fra_tokenizer, fra_length, train_ds[:, 1])\n",
    "X_test = encode_sequences(fra_tokenizer, fra_length, test_ds[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SK5l3P7mHHSC"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "model = models.load_model('translator_weights_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tdo3GeVODiQE"
   },
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    " \n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [np.argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68838,
     "status": "ok",
     "timestamp": 1545651229927,
     "user": {
      "displayName": "Victor Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/-nVI9QJN9Uk4/AAAAAAAAAAI/AAAAAAAAACk/6lxg_oQk5wU/s64/photo.jpg",
      "userId": "17175242154365071029"
     },
     "user_tz": -480
    },
    "id": "nyF49Li51i_L",
    "outputId": "ff40403e-88ce-49ab-b71a-078219d37006"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集：\n",
      "源语句=[il detient une video], 目标语句=[he has a video], 预测语句=[he has a video]\n",
      "源语句=[pourraistu maider], 目标语句=[can you help me], 预测语句=[can you help me]\n",
      "源语句=[soyez respectueuses], 目标语句=[be respectful], 预测语句=[be respectful]\n",
      "源语句=[vous etes naif], 目标语句=[youre naive], 预测语句=[youre naive]\n",
      "源语句=[il travaille a peine], 目标语句=[he hardly works], 预测语句=[he hardly works]\n",
      "源语句=[montrezmoi], 目标语句=[show me], 预测语句=[show me]\n",
      "源语句=[je dois y aller], 目标语句=[i must go], 预测语句=[i must to go]\n",
      "源语句=[quelle equipe], 目标语句=[what a team], 预测语句=[what a team]\n",
      "源语句=[nous sommes en retard], 目标语句=[we are late], 预测语句=[were late]\n",
      "源语句=[appelle la securite], 目标语句=[call security], 预测语句=[call security]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.08884628871460039\n",
      "BLEU-2: 0.28372048975984243\n",
      "BLEU-3: 0.4514308812293625\n",
      "BLEU-4: 0.5070098604719333\n",
      "测试集：\n",
      "源语句=[venez la], 目标语句=[come here], 预测语句=[come back]\n",
      "源语句=[laissemoi sortir], 目标语句=[let me out], 预测语句=[let me out]\n",
      "源语句=[ne soyez pas mesquin], 目标语句=[dont be mean], 预测语句=[dont be mean]\n",
      "源语句=[vous devez le faire], 目标语句=[you must do it], 预测语句=[you need do it]\n",
      "源语句=[ty rendrastu], 目标语句=[will you go], 预测语句=[shut you serious]\n",
      "源语句=[elles ont refuse], 目标语句=[they refused], 预测语句=[they refused]\n",
      "源语句=[ils nous ont trouvees], 目标语句=[they found us], 预测语句=[they found us]\n",
      "源语句=[jadore cuisiner], 目标语句=[i love cooking], 预测语句=[i love baking]\n",
      "源语句=[tom semble perdu], 目标语句=[tom seems lost], 预测语句=[tom lost]\n",
      "源语句=[avaisje tort], 目标语句=[was i wrong], 预测语句=[am was i]\n",
      "BLEU-1: 0.07970902428748444\n",
      "BLEU-2: 0.26403639977318066\n",
      "BLEU-3: 0.4263107912362802\n",
      "BLEU-4: 0.48055373499039605\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def test_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('源语句=[{}], 目标语句=[{}], 预测语句=[{}]'.format(raw_src, raw_target, translation))\n",
    "        actual.append(raw_target.split())\n",
    "        predicted.append(translation.split())\n",
    "    print('BLEU-1: {}'.format(corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))))\n",
    "    print('BLEU-2: {}'.format(corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))))\n",
    "    print('BLEU-3: {}'.format(corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))))\n",
    "    print('BLEU-4: {}'.format(corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))))\n",
    "\n",
    "print('训练集：')\n",
    "test_model(model, eng_tokenizer, X_train, train_ds)\n",
    "print('测试集：')\n",
    "test_model(model, eng_tokenizer, X_test, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cYbe2kjb1jCE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bhAs2GOP2vXE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r4HN7sTB2vZm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9pvvAi9B2vcI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Language_Translation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
